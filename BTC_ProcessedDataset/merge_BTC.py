"""
Script de fusion des donn√©es BTC ProcessedDataset
Fusionne tous les fichiers parquet des sous-dossiers (par ann√©e)
Les donn√©es sont d√©j√† nettoy√©es et contiennent des features suppl√©mentaires
"""

import pandas as pd
from pathlib import Path
import sys

def merge_btc_processed_data():
    """
    Fusionne tous les fichiers parquet du BTC_ProcessedDataset.
    Les donn√©es sont organis√©es par dossiers d'ann√©es, chaque fichier faisant ~76MB.
    """
    # R√©pertoire courant (BTC_ProcessedDataset)
    current_dir = Path(__file__).parent
    
    print("=" * 70)
    print("FUSION DES DONN√âES BTC PROCESSED (2021-2024)")
    print("=" * 70)
    
    # Parcourir tous les sous-dossiers
    child_dirs = [d for d in current_dir.iterdir() if d.is_dir()]
    
    if not child_dirs:
        print("‚ùå Aucun sous-dossier trouv√© !")
        return None
    
    print(f"\nüìÅ {len(child_dirs)} dossier(s) trouv√©(s): {[d.name for d in child_dirs]}")
    
    dataframes = []
    total_files = 0
    
    # Parcourir chaque dossier et charger tous les fichiers parquet
    for year_dir in sorted(child_dirs):
        print(f"\nüìÇ Traitement du dossier: {year_dir.name}")
        
        parquet_files = list(year_dir.glob("*.parquet"))
        
        if not parquet_files:
            print(f"   ‚ö†Ô∏è  Aucun fichier parquet trouv√© dans {year_dir.name}")
            continue
        
        print(f"   ‚Üí {len(parquet_files)} fichier(s) parquet trouv√©(s)")
        
        dir_dataframes = []
        for file in sorted(parquet_files):
            try:
                df = pd.read_parquet(file)
                dir_dataframes.append(df)
                print(f"      ‚úì {file.name}: {len(df):,} lignes")
            except Exception as e:
                print(f"      ‚ùå Erreur lors de la lecture de {file.name}: {e}")
        
        if dir_dataframes:
            # Concat√©ner les fichiers du dossier
            year_df = pd.concat(dir_dataframes, ignore_index=True)
            print(f"   ‚úì Total {year_dir.name}: {len(year_df):,} lignes")
            dataframes.append(year_df)
            total_files += len(dir_dataframes)
    
    if not dataframes:
        print("\n‚ùå Aucune donn√©e √† fusionner!")
        return None
    
    print(f"\nüîÑ Fusion de {total_files} fichiers parquet...")
    
    # Concat√©nation de tous les dataframes
    merged_df = pd.concat(dataframes, ignore_index=True)
    print(f"   ‚úì Total apr√®s concat√©nation: {len(merged_df):,} lignes")
    
    # Tri chronologique
    if 'datetime' in merged_df.columns:
        print("\nüîÑ Tri chronologique par 'datetime'...")
        merged_df = merged_df.sort_values('datetime').reset_index(drop=True)
        print("   ‚úì Donn√©es tri√©es")
    elif 'Open_Time' in merged_df.columns:
        print("\nüîÑ Tri chronologique par 'Open_Time'...")
        merged_df = merged_df.sort_values('Open_Time').reset_index(drop=True)
        print("   ‚úì Donn√©es tri√©es")
    
    # Suppression des doublons √©ventuels
    initial_len = len(merged_df)
    merged_df = merged_df.drop_duplicates()
    if len(merged_df) < initial_len:
        print(f"   ‚úì {initial_len - len(merged_df):,} doublons supprim√©s")
    
    # Informations sur les colonnes
    print(f"\nüìä Informations sur le dataset:")
    print(f"   Nombre de lignes: {len(merged_df):,}")
    print(f"   Nombre de colonnes: {len(merged_df.columns)}")
    print(f"   Colonnes: {list(merged_df.columns)}")
    
    # Aper√ßu des dates
    if 'datetime' in merged_df.columns:
        print(f"\nüìÖ P√©riode couverte:")
        print(f"   D√©but: {merged_df['datetime'].min()}")
        print(f"   Fin:   {merged_df['datetime'].max()}")
    elif 'Open_Time' in merged_df.columns:
        print(f"\nüìÖ P√©riode couverte (Open_Time):")
        print(f"   D√©but: {pd.to_datetime(merged_df['Open_Time'].min(), unit='ms')}")
        print(f"   Fin:   {pd.to_datetime(merged_df['Open_Time'].max(), unit='ms')}")
    
    # Statistiques basiques
    print(f"\nüìà Aper√ßu des donn√©es (prix):")
    if 'Close' in merged_df.columns:
        print(f"   Close - Min: ${merged_df['Close'].min():.2f}")
        print(f"   Close - Max: ${merged_df['Close'].max():.2f}")
        print(f"   Close - Moyenne: ${merged_df['Close'].mean():.2f}")
    
    # Sauvegarde
    output_path = current_dir / "BTC_processed_merged_2017_2024.parquet"
    print(f"\nüíæ Sauvegarde vers: {output_path}")
    merged_df.to_parquet(output_path, index=False)
    print(f"   ‚úì Fichier sauvegard√©: {output_path.name}")
    print(f"   Taille: {len(merged_df):,} lignes, {len(merged_df.columns)} colonnes")
    
    print("\n" + "=" * 70)
    print("‚úÖ FUSION TERMIN√âE AVEC SUCC√àS!")
    print("=" * 70)
    
    return merged_df


def explore_data(df: pd.DataFrame):
    """Affiche des informations d√©taill√©es sur le dataset"""
    print("\n" + "=" * 70)
    print("EXPLORATION DES DONN√âES")
    print("=" * 70)
    
    print(f"\nüîç Shape: {df.shape}")
    print(f"\nüìã Info du DataFrame:")
    print(df.info())
    
    print(f"\nüìä Statistiques descriptives:")
    print(df.describe())
    
    print(f"\nüëÅÔ∏è  Aper√ßu des premi√®res lignes:")
    print(df.head())
    
    print(f"\nüëÅÔ∏è  Aper√ßu des derni√®res lignes:")
    print(df.tail())
    
    # V√©rifier les valeurs manquantes
    missing = df.isnull().sum()
    if missing.any():
        print(f"\n‚ö†Ô∏è  Valeurs manquantes:")
        print(missing[missing > 0])
    else:
        print(f"\n‚úÖ Aucune valeur manquante")


if __name__ == "__main__":
    # Fusionner les donn√©es
    df = merge_btc_processed_data()
    
    # Explorer si l'option est pass√©e
    if df is not None and len(sys.argv) > 1 and sys.argv[1] == '--explore':
        explore_data(df)

